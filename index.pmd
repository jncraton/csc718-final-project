% GPU Programming
% Jonathan Craton
% December 10th, 2019

What is GPU Programming
=======================

GPU programming involves writing software that is executed at least in part on the system's Graphics Processing Unit, or GPU.

Traditionally, GPU programming involved interfacing with various stages of the graphic processing pipeline [1]. This could be as basic as passing polygons to the GPU to be rendered using a set of parameters, but as GPUs evolved to be more powerful, programmers were given more tools to interface with the graphics pipeline [2].

Over time, programmers were given the ability execute small programs at various points in the graphics pipeline. These programs are referred to as shaders. Different shaders could interact with the graphics pipeline at various stages. The types of shaders available include vertex shaders, fragment shaders, geometry shaders, and tessellation shaders.

Computer 3D graphics as implemented by traditional GPUs is an embarrassingly parallel algorithm. Vertexes, polygons, and individual pixels (fragments) can generally be processed independently. Therefore, much of the rendering speed of a modern GPU is based on the number of parallel cores that it possesses. Many modern GPUs have thousands of cores allowing them to work on thousands of floating point operations simultaneously.

The desire to improve physics simulations in games [3], and use of GPUs for machine learning and other FP-intensive tasks led to the GPU opening up as a more general purpose computing device. Drivers and graphics cards now usually provide the ability to run general purpose code on the GPU. This is the core of modern GPU programming.

Importance of GPU Programming
-----------------------------

GPU programming has become critical in certain industries. Any application which can execute a large number of calculations in parallel will likely see significantly improved performance when executed on a GPU [4]. Problems such as physics simulations, finite element modeling, machine learning, video encoding, and many others all benefit dramatically from the sort of parallelism available on a modern GPU.

Single-threaded performance is still very important for common computing tasks. Many daily tasks can't be easily paralellized, or it would significantly more complicated to do so. For this reason, the CPUs in modern devices have found a balance that tries to maximize single-threaded efficiency while also providing additional cores, SIMD instructions, and multiple threads per core for workloads that can be run in parallel.

Unfortunately, we are hitting many challenges in improving single threaded performance. Increasing CPU clocks requires additional power. This power is expressed as heat and must be dissipated in order to avoid damaging the CPU. This places a hard limit on how fast our chips can run. In addition, it is becoming more difficult to shrink transistors, thus placing constraints on the level of advanced designs that we can implement in silicon. In short, we are at the stage of diminishing returns on single-threaded CPU performance [5].

GPUs are not constrained by a need to maximize single threaded performance. They often run their cores at significantly reduced clock compared CPUs, which allows them to use less energy, produce less heat, and generally be packed much more tightly onto a die.

This has made GPUs the best option for vector processing in modern computer architectures. If we want to have access to these best-in-class vector processors, we must be able to write programs that can execute on them.

While GPUs have historically been associated nearly exclusively with gaming, this is no longer the case. A modern workstation now likely contains a strong GPU that can be used for simulation or machine learning tasks in additional to actually rendering 3D graphics of various kinds.

GPU Programming Support and Frameworks
--------------------------------------

Over the years, GPUs have provided various levels of programming support. In the early days, it was only possible to use GPUs for graphics applications. Over time, more and more tools have been provided for developers to use GPUs for general purpose computing.

Here's an example of the a modern unified graphics and computing architecture found in the NVIDIA GeForce 8800:

![GeForce 8800 [10]](media/8800.png)

We can see here that a modern GPU is organized as a hierarchy of compute units and associated RAM. We see that there are Texture Processing Clusters (TPCs) that manage a number of Stream Multiprocessors (SM) that in turn manage a number of Streaming Processors (SPs). This massively parallel architecture allows modern GPUs to performs hundreds or thousands of GFLOPS, despite being generally clocked a bit lower than the main CPU.

Since the GeForce 8800 shown above is a little dated, I also wanted to explore how much performance has improved by comparing it against the GPU in my current workstation. The 8800 GT contains 112 Stream Processors, while the Quadro P3200 in the laptop on my desk at work contains 1,792 CUDA cores [11]. That's over a 10 fold increase in around a decade. The 8800 also tops out at 1GB of RAM, while my more modern GPU contains 6GB on board. Memory bandwidth also increased from 57 GB/s to 168 GB/s. While this is a great improvement, it's also worth noting that this is not an apples to apples comparison, as the 8800 GT was a desktop GPU with a TDP of 125W, while my P3200 is a mobile GPU with a TDP of 78W.

As another point of reference, a modern GPGPU compute card, such as the NVIDIA V100, can max out at around 14 TFLOPS, while the 8800 GT could manage around 0.3 TFLOPS. Most high-end desktop CPUs can't sustain anything over 0.15 TFLOPS. By making use of modern GPGPU programming, we can see at least 2 orders of magnitude faster computational performance.

### CUDA

CUDA is a programming framework developed by NVIDIA specifically for NVIDIA CPUs [6]. According to NVIDIA, this was the first toolkit available for general purpose computing on GPUs.

Broadly speaking, CUDA provides programmers with tools to execute functions over vectors in their codebases using the GPU. This model is very flexible, as it means any array of numbers that the programmer can create can be quickly evaluated in parallel using the GPU.

### OpenCL


Like CUDA, OpenCL provides a way for developers to access the GPU from their programming environment [8]. OpenCL is distinctive for being cross-platform (it can be run on a number of different GPUs) as well as being free and open source software.

### OpenACC

Like OpenCL, OpenACC strives to provide a parallel programming environment across devices [9]. OpenACC achieves this by providing a higher-level interface that can be compiled down to one of several different target environments (CUDA, OpenCL, etc).

CUDA Programming
----------------

Programming in CUDA is based around the concept of kernels [7]. Kernels are specialized C functions that can be executed in parallel in CUDA threads.

Kernels will take as parameters pointers to arrays of values to be operated upon. The kernels will have access to the built-in threadIdx variable that allows the running kernel to identify the appropriate value in the array to operate upon. The threadIdx variable provides 3 components that can be used to iterate over one, two, and three dimensional data. Here's is an example from the documentation [7]:

```c
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
                       float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation with one block of N * N * 1 threads
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```

One of the primary differences between CPU and GPU programming is the memory model. A CPU shares physical RAM with all other processes running on the computer, and shares RAM between threads within a single program. Most operating systems also provide mechanisms for sharing memory between processes.

A GPU has memory that is physically separate from the rest of the system RAM. Data cannot be shared in the traditional way. We actually have to copy data to the GPU, operate on it, and then copy results back to the host RAM. The term **device RAM** is used to refer to memory and the GPU and **host RAM** refers to the native RAM on the device that is directly accessible to the CPU.

Since CUDA 6.0, NVIDIA has provided a Unified Memory model that allows a single block of RAM to be shared between the host and device. This model handles the complexity of transferring data between GPU and CPU RAM and allows for a single logically shared address space to be used. This greatly simplifies programming and make it much simpler to port code between CUDA environments and non-CUDA environments.

This shared memory model is one of the significant advantages of CUDA programming. Its primary limitation is that it is only usable on NVIDIA GPUs, as it is a proprietary framework developed and provided by NVIDIA. Overall, CUDA programming provides an effective and relatively straigtforward way for programmers to write code that can execute in parallel on an NVIDIA GPU.

Programming Project
===================

Introduction
------------

I've chosen to explore building and optimizing an n-body physics simulator. The particular problem I've chosen to explore is the creation of moons due to asteroid impact. I have neither the expertise nor compute time available to really do this properly, but this project gave me away to explore paralell computing concept in a fun environment.

Wikipedia describes N-body simulation as:

> In physics and astronomy, an N-body simulation is a simulation of a dynamical system of particles, usually under the influence of physical forces, such as gravity

Research Question
-----------------

- In October, [we discovered 20 new moons of Saturn](http://www.astronomy.com/news/2019/10/20-new-moons-discovered-orbiting-saturn).
- How does a large body disintigrating on impact with another large body form moons or ring systems around a planetoid?

Algorithm
---------

We begin by creating many (N) bodies with initial condition suitable for the physical process we want to study. We then iterativly update these bodies to simulate their motion. The steps for the update process are:

1. Apply gravitational force to each body from every other body - O(N^2)
2. Search for collisions and merge objects - O(N^2)
3. Update positions using new velocity vectors - O(N)

My program outputs data files at configurable steps in the simulation process. The included makefile can be used to convert these into animations like the following:

![](media/animation.gif)

Note that the reported mass is the total mass in orbit. Any orbits that decay into the planetoid have their mass added to it. Mass is conserved in this simulation.

Building and Running
--------------------

### Building

The code for this project is available in the `src` directory. It can be built from there using the included makefile.

Note that the default task in the makefile generates 3 animated GIFs from the results generated by each implementation. Additional dependencies are required for this. The individual binaries can be built by calling `make dist/seq`, `make dist/omp`, and `make dist/cuda`.

#### Software Requirements

##### Core Application

- gcc
- GNU make
- nvcc from the NVidia Toolkit

##### GIF Animation Generation

After data files have been generated, the following are needed to convert them to GIF animations.

- Python 3
- imageio (`pip3 install -r requirements.txt` from src dir)
- gifsicle (`apt install gifsicle` on Debian-based operating systems)

#### Hardware Requirements

- Intel Ivy Bridge or later CPU
- NVidia GPU supporting Shader Model 6.0 or higher

### Running

The default make task will build all executables, generate result files, and use the results to build the final animations. This process can be started by running `make`.

### Output

The default make task will store build output in the following locations for each implementation:

- `dist/seq-results`
- `dist/omp-results`
- `dist/cuda-results`

The directories will be populated with csv files representing time steps within the simulation.

These output files are merged by `render.py` into nice animated GIFs by the build process.

The animated GIFs will be placed in the `media` directory.

Test Hardware
-------------

This code was tested and benchmarked on a workstation with the following meaningful specs:

Component   Model                 Specs
----------  --------------------  -----------------------------------------------
CPU         Intel Core i7-8850H   6 Cores, 12 Threads, 2.6GHz clock
GPU         NVidia P3200          14 Shader Modules, 1792 Shader Units, 6 GB RAM

Sequential Implementation
-------------------------

I began this project by creating a sequential program to implement this simulation. Using the configured parameters, it runs in about 120 seconds on my system. Using that code as a starting point, I began the process of converting and opimizing it to make better use of the resources of a modern paralell computer.

The sequential implementation can be found in `src/seq.c` and the core is something like this:

```c
for (i = 1; i < N; i++)
  for (j = 0; j < N; j++)
      bodies[i].velocity += {Acceleration from bodies[j]};

for (j = 0; j < N; j++)
  bodies[i].position += bodies[i].velocity * delta_time;
```

The actual code is a bit more complex than that, as it includes a number of optimizations and handles collision detection, but this demonstrates the core of the algorithm.

OpenMP
------

I used OpenMP to boost performance using a shared memory model. By running the main loop in parallel, I was able to go from about 120 seconds of runtime to about 31 seconds using 12 threads (6 cores with 2 threads each). This represents about a 4x speedup from parallelism. 

The code for the OpenMP implementation is found in `src/omp.c` and the core code has been adjusted to look something like this:

```c
#pragma omp parallel for private(j)
for (i = 1; i < N; i++)
  for (j = 0; j < N; j++)
      bodies[i].velocity += {Acceleration from bodies[j]};
```

This implementation took only a few minutes to get up and running and really shows the value of OpenMP as a tool.

CUDA
----

The bulk of my work in this project involved learning CUDA and adjusting my program to appropraiately make use of the GPU.

### Proof of Concept

My CUDA work began by simply working to get the problem to run on my GPU. Having not used my workstation for GPU programming previously, I wanted to challenge myself to get the code running locally rather than on the provided AWS instance. I ended up spending a non-trivial amount of time getting drivers installed and X configured properly so that I could use my Intel GPU for graphics while having my NVidia GPU active and ready to be used for compute processing. This setup allows me to continue working without any graphical hiccups while my GPU is busy with computation.

I won't go into the detail of every step in my process (this is all available in the Git history if you are curious), but I did want to highlight that I initially implemented this using only a single block and a single thread, e.g.:

```c
update_from_gravity<<<1,1>>>(cuda_bodies, cuda_N);
```

This allowed me to simply get up and running using only slight modifications to my sequential code. The performance of this code is not worth mentioning, and I never executed it in a way that would make it comparable to my other implementations.

CUDA-enabled devices do not share memory with the host system, so we have to allocate device memory and fill it from the host. I achieved this by calling:

```c
cudaMalloc(&cuda_bodies, N * sizeof(Body))
```

This allocates the memory. I then copy the data to my GPU using:

```c
cudaMemcpy(cuda_bodies, bodies, N * sizeof(Body), cudaMemcpyHostToDevice)
```

Once the simulation has completed the selected number of iterations, it copies the data back to host system for processing:

```c
cudaMemcpy(bodies, cuda_bodies, N * sizeof(Body), cudaMemcpyDeviceToHost)
```

### Multiple Threads

It is a bit silly to write single-threaded code to run on a GPU, so the next step in my process was to implement multithreading. I did this by calling into my kernel as follows:

```c
update_from_gravity<<<1,1024>>>(cuda_bodies, cuda_N);
```

I could then adjust my loop conditions to have each thread perform only every 1024th operation something like the following:

```c
for (j = threadIdx.x; j < *N; j+=blockDim.x) {
```

This produced significant speedup, but I could see from very basic analysis that it was still inefficient. I noticed quite simply that my GPU was not maxing out it's power profile (It should be able to pull 80W):

    +-------------------------------+
    | NVIDIA-SMI 440.36             |
    |-------------------------------+
    | GPU  Name        Persistence-M|
    | Fan  Temp  Perf  Pwr:Usage/Cap|
    |===============================+
    |   0  Quadro P3200        Off  |
    | N/A   55C    P0    36W /  N/A |
    +-------------------------------+

    +-------------------------------+
    | Processes:                    |
    |  GPU       PID   Type   Proc  |
    |===============================+
    |    0      2717      C   cuda  |
    +-------------------------------+

### Multiple Blocks

CUDA kernels introduce a concept called blocks in order to allow code to run over a number of Shader Modules. With a single block, all code is executed by only a single SM. This is very inefficient, and we want to add additional blocks.

![CUDA Kernel Blocks](media/cuda-kernel-blocks.png)

We can use multiple blocks for a kernel when calling it so that it is able to be processed by multiple shader modules:

```c
update_from_gravity<<<N,1>>>(cuda_bodies, cuda_N)
```

The code now executes in 36 seconds. This produces a significant speed up over the previous version, but we can still do much better.

### Multiple Blocks with Multiple Threads

We can use multiple blocks and multiple threads per block like this:

```c
update_from_gravity<<<N-1,128>>>(cuda_bodies, cuda_N);
```

Now we see the GPU hitting higher TDP as we are now using all shader modules:

    +-------------------------------+
    | NVIDIA-SMI 440.36             |
    |-------------------------------+
    | GPU  Name        Persistence-M|
    | Fan  Temp  Perf  Pwr:Usage/Cap|
    |===============================+
    |   0  Quadro P3200        Off  |
    | N/A   55C    P0    67W /  N/A |
    +-------------------------------+

    +-------------------------------+
    | Processes:                    |
    |  GPU       PID   Type   Proc  |
    |===============================+
    |    0      3217      C   cuda  |
    +-------------------------------+

This reduces our runtime to 5.1 seconds, a 6x speedup from our OpenMP implementation.

### Removing Explicit Loops

CUDA also provides us with a way to use multidimensional blocks. This is the proper way to execute our code so that we aren't running explicit loops on the GPU. Here's how this is called:

```c
int threads_per_block = 1024;
dim3 grid(N-1, N / threads_per_block);
update_from_gravity<<<grid,threads_per_block>>>(cuda_bodies, cuda_N);
```

We then update our kernel to calculate the appropriate memory indices using CUDA-supplied indices:

```c
int i = blockIdx.x + 1;
int j = blockIdx.y * blockDim.y + threadIdx.x;
```

This improves our runtime to 3.8 seconds.

### Additional Improvements

While I was happy to handily outperform my CPU cores with CUDA, I believe that there may still be room for improvement. This problem is tricky to optimize for several reasons:

1. N decreases as the simulation progresses, so it is not possible preselect values to optimally consume GPU resources.
2. A branch is required when searching to handle collision detection. This shouldn't be a massive hit, as it should be an easy branch to predict, but it does still come at some cost.

My current implementation also has one additional branch to ensure that we aren't comparing objects to themselves. It would be lovely if this could be removed. I also currently make use of atomic operations. It is possible that there is a way to order operations and perform syncronization to eliminate these.

Performance Comparison
----------------------

Here's a summary of the performance of these implenetations:

Implementation                Runtime   Speedup
----------------------------  --------- -------
Sequential                    120 s     1x
OpenMP                        31 s      3.8x
CUDA (1 thread per block)     36 s      3.3x
CUDA (128 threads per block)  5.1 s     23.5x
CUDA (no explicit loops)      3.8 s     31.5x

We see that my final CUDA implementation outperforms my sequential code by over 30x.

Final Thoughts
--------------

This project provided a great platform for me to learn about parallel computing. The problem I explored had obvious sections that could be improved via parallelism, but it was also challenging to optimize those gains due to a dynamic N and a few required branches.

I feel confident that I could use CUDA or other GPU programming environments to create optimized algorithms in the future.

Appendix A - Project Deliverables
=================================

- Part 1 Report - The first section of this document
- Part 2 
    - Source Code - Located in the `src` directory or on [Github](https://github.com/jncraton/csc718-final-project/tree/master/src)
    - Final Report - This document
    - Presentation slides - See [slides.html](slides.html). Note that this presentation uses a 2D grid of slides. Slides can be both next to one another or under one another. Navigation uses the arrow keys. ESC can be used to see the whole grid. This is just reveal.js if you've worked with it before.

References
==========

[1] Shreiner, Dave, Tom Davis, Mason Woo, and Jackie Neider. "OpenGL Programming Guide: The Official Guide to Learning OpenGL, Version 2.1." (2008).

[2] Marroquim, Ricardo, and André Maximo. "Introduction to GPU Programming with GLSL." In 2009 Tutorials of the XXII Brazilian Symposium on Computer Graphics and Image Processing, pp. 3-16. IEEE, 2009.

[3] Wittenbrink, Craig M., Emmett Kilgariff, and Arjun Prabhu. "Fermi GF100 GPU architecture." IEEE Micro 31, no. 2 (2011): 50-59.

[4] Che, Shuai, Michael Boyer, Jiayuan Meng, David Tarjan, Jeremy W. Sheaffer, and Kevin Skadron. "A performance study of general-purpose applications on graphics processors using CUDA." Journal of parallel and distributed computing 68, no. 10 (2008): 1370-1380.

[5] Theis, Thomas N., and H-S. Philip Wong. "The end of moore's law: A new beginning for information technology." Computing in Science & Engineering 19, no. 2 (2017): 41.

[6] “CUDA Zone.” NVIDIA Developer, September 12, 2019. https://developer.nvidia.com/cuda-zone. 

[7] “CUDA C Programming Guide.” NVIDIA Developer Documentation. Accessed September 20, 2019. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html. 

[8] “OpenCL - The Open Standard for Parallel Programming of Heterogeneous Systems.” The Khronos Group, July 21, 2013. https://www.khronos.org/opencl/

[9] “About OpenACC.” Open ACC. Accessed September 20, 2019. https://www.openacc.org/about. 

[10] Nickolls, John, and David Kirk. "Graphics and computing GPUs." Computer Organization and Design: The Hardware/Software Interface, DA Patterson and JL Hennessy, 4th ed., Morgan Kaufmann (2009): A2-A77.

[11] "NVIDIA Professional Graphics Solutions". NVIDIA. Accessed October 11, 2019. https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/documents/quadro-mobile-line-card-n18-11x8.5-r4-hr.pdf
