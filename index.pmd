% GPU Programming
% Jonathan Craton
% October 15th, 2019

What is GPU Programming
=======================

GPU programming involves writing software that is executed at least in part on the system's Graphics Processing Unit, or GPU.

Traditionally, GPU programming involved interfacing with various stages of the graphic processing pipeline [1]. This could be as basic as passing polygons to the GPU to be rendered using a set of parameters, but as GPUs evolved to be more powerful, programmers were given more tools to interface with the graphics pipeline [2].

Over time, programmers were given the ability execute small programs at various points in the graphics pipeline. These programs are referred to as shaders. Different shaders could interact with the graphics pipeline at various stages. The types of shaders available include vertex shaders, fragment shaders, geometry shaders, and tessellation shaders.

Computer 3D graphics as implemented by traditional GPUs is an embarrassingly parallel algorithm. Vertexes, polygons, and individual pixels (fragments) can generally be processed independently. Therefore, much of the rendering speed of a modern GPU is based on the number of parallel cores that it possesses. Many modern GPUs have thousands of cores allowing them to work on thousands of floating point operations simultaneously.

The desire to improve physics simulations in games [3], and use of GPUs for machine learning and other FP-intensive tasks led to the GPU opening up as a more general purpose computing device. Drivers and graphics cards now usually provide the ability to run general purpose code on the GPU. This is the core of modern GPU programming.

Importance of GPU Programming
=============================

GPU programming has become critical in certain industries. Any application which can execute a large number of calculations in parallel will likely see significantly improved performance when executed on a GPU [4]. Problems such as physics simulations, finite element modeling, machine learning, video encoding, and many others all benefit dramatically from the sort of parallelism available on a modern GPU.

Single-threaded performance is still very important for common computing tasks. Many daily tasks can't be easily paralellized, or it would significantly more complicated to do so. For this reason, the CPUs in modern devices have found a balance that tries to maximize single-threaded efficiency while also providing additional cores, SIMD instructions, and multiple threads per core for workloads that can be run in parallel.

Unfortunately, we are hitting many challenges in improving single threaded performance. Increasing CPU clocks requires additional power. This power is expressed as heat and must be dissipated in order to avoid damaging the CPU. This places a hard limit on how fast our chips can run. In addition, it is becoming more difficult to shrink transistors, thus placing constraints on the level of advanced designs that we can implement in silicon. In short, we are at the stage of diminishing returns on single-threaded CPU performance [5].

GPUs are not constrained by a need to maximize single threaded performance. They often run their cores at significantly reduced clock compared CPUs, which allows them to use less energy, produce less heat, and generally be packed much more tightly onto a die.

This has made GPUs the best option for vector processing in modern computer architectures. If we want to have access to these best-in-class vector processors, we must be able to write programs that can execute on them.

While GPUs have historically been associated nearly exclusively with gaming, this is no longer the case. A modern workstation now likely contains a strong GPU that can be used for simulation or machine learning tasks in additional to actually rendering 3D graphics of various kinds.

GPU Programming Support and Frameworks
=====================================

Over the years, GPUs have provided various levels of programming support. In the early days, it was only possible to use GPUs for graphics applications. Over time, more and more tools have been provided for developers to use GPUs for general purpose computing.

Here's an example of the a modern unified graphics and computing architecture found in the NVIDIA GeForce 8800:

![GeForce 8800 [10]](media/8800.png)

We can see here that a modern GPU is organized as a hierarchy of compute units and associated RAM. We see that there are Texture Processing Clusters (TPCs) that manage a number of Stream Multiprocessors (SM) that in turn manage a number of Streaming Processors (SPs). This massively parallel architecture allows modern GPUs to performs hundreds or thousands of GFLOPS, despite being generally clocked a bit lower than the main CPU.

Since the GeForce 8800 shown above is a little dated, I also wanted to explore how much performance has improved by comparing it against the GPU in my current workstation. The 8800 GT contains 112 Stream Processors, while the Quadro P3200 in the laptop on my desk at work contains 1,792 CUDA cores [11]. That's over a 10 fold increase in around a decade. The 8800 also tops out at 1GB of RAM, while my more modern GPU contains 6GB on board. Memory bandwidth also increased from 57 GB/s to 168 GB/s. While this is a great improvement, it's also worth noting that this is not an apples to apples comparison, as the 8800 GT was a desktop GPU with a TDP of 125W, while my P3200 is a mobile GPU with a TDP of 78W.

As another point of reference, a modern GPGPU compute card, such as the NVIDIA V100, can max out at around 14 TFLOPS, while the 8800 GT could manage around 0.3 TFLOPS. Most high-end desktop CPUs can't sustain anything over 0.15 TFLOPS. By making use of modern GPGPU programming, we can see at least 2 orders of magnitude faster computational performance.

CUDA
----

CUDA is a programming framework developed by NVIDIA specifically for NVIDIA CPUs [6]. According to NVIDIA, this was the first toolkit available for general purpose computing on GPUs.

Broadly speaking, CUDA provides programmers with tools to execute functions over vectors in their codebases using the GPU. This model is very flexible, as it means any array of numbers that the programmer can create can be quickly evaluated in parallel using the GPU.

OpenCL
------

Like CUDA, OpenCL provides a way for developers to access the GPU from their programming environment [8]. OpenCL is distinctive for being cross-platform (it can be run on a number of different GPUs) as well as being free and open source software.

OpenACC
-------

Like OpenCL, OpenACC strives to provide a parallel programming environment across devices [9]. OpenACC achieves this by providing a higher-level interface that can be compiled down to one of several different target environments (CUDA, OpenCL, etc).

CUDA Programming
================

Programming in CUDA is based around the concept of kernels [7]. Kernels are specialized C functions that can be executed in parallel in CUDA threads.

Kernels will take as parameters pointers to arrays of values to be operated upon. The kernels will have access to the built-in threadIdx variable that allows the running kernel to identify the appropriate value in the array to operate upon. The threadIdx variable provides 3 components that can be used to iterate over one, two, and three dimensional data. Here's is an example from the documentation [7]:

```c
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
                       float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation with one block of N * N * 1 threads
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```

One of the primary differences between CPU and GPU programming is the memory model. A CPU shares physical RAM with all other processes running on the computer, and shares RAM between threads within a single program. Most operating systems also provide mechanisms for sharing memory between processes.

A GPU has memory that is physically separate from the rest of the system RAM. Data cannot be shared in the traditional way. We actually have to copy data to the GPU, operate on it, and then copy results back to the host RAM. The term **device RAM** is used to refer to memory and the GPU and **host RAM** refers to the native RAM on the device that is directly accessible to the CPU.

Since CUDA 6.0, NVIDIA has provided a Unified Memory model that allows a single block of RAM to be shared between the host and device. This model handles the complexity of transferring data between GPU and CPU RAM and allows for a single logically shared address space to be used. This greatly simplifies programming and make it much simpler to port code between CUDA environments and non-CUDA environments.

This shared memory model is one of the significant advantages of CUDA programming. Its primary limitation is that it is only usable on NVIDIA GPUs, as it is a proprietary framework developed and provided by NVIDIA. Overall, CUDA programming provides an effective and relatively straigtforward way for programmers to write code that can execute in parallel on an NVIDIA GPU.

Project Part II Proposal
========================

I have chosen to select a problem to explore various parallel programming approaches. I will likely experiment with an n-body physics simulation using the following techniques:

1. Sequential programming
2. Parallel programming using OpenMP
3. GPU programming likely using OpenCL or OpenACC

I intend to complete this project without a group or partner.

Introduction
------------

I've chosen to explore building and optimizing an n-body physics simulator. The particular problem I've chosen to explore is the creation of moons due to asteroid impact. I have neither the expertise nor compute time available to really do this properly, but this project gave me away to explore paralell computing concept in a fun environment.

My program outputs data files at configurable steps in the simulation process. The included makefile can be used to convert these into animation like the following:

![](media/animation.gif)

### Sequential

I began this project by creating a simply sequential program to implement this simulation. Using the configured parameters, it runs in about 3.5 seconds on my system. Using that code as a starting point, I began the process of converting and opimizing it to make better use of the resources of a modern paralell computer.

### OpenMP

I used OpenMP to boost performance using a shared memory model. By running the main loop in parallel, I was able to go from about 3.5 seconds of runtime to about .7 seconds using 12 threads (6 cores with 2 threads each). This represents roughly a 5x speedup from simple parallelism. In truth, this section of work took about 15 seconds and really shows the value of OpenMP as a tool.

References
==========

[1] Shreiner, Dave, Tom Davis, Mason Woo, and Jackie Neider. "OpenGL Programming Guide: The Official Guide to Learning OpenGL, Version 2.1." (2008).

[2] Marroquim, Ricardo, and André Maximo. "Introduction to GPU Programming with GLSL." In 2009 Tutorials of the XXII Brazilian Symposium on Computer Graphics and Image Processing, pp. 3-16. IEEE, 2009.

[3] Wittenbrink, Craig M., Emmett Kilgariff, and Arjun Prabhu. "Fermi GF100 GPU architecture." IEEE Micro 31, no. 2 (2011): 50-59.

[4] Che, Shuai, Michael Boyer, Jiayuan Meng, David Tarjan, Jeremy W. Sheaffer, and Kevin Skadron. "A performance study of general-purpose applications on graphics processors using CUDA." Journal of parallel and distributed computing 68, no. 10 (2008): 1370-1380.

[5] Theis, Thomas N., and H-S. Philip Wong. "The end of moore's law: A new beginning for information technology." Computing in Science & Engineering 19, no. 2 (2017): 41.

[6] “CUDA Zone.” NVIDIA Developer, September 12, 2019. https://developer.nvidia.com/cuda-zone. 

[7] “CUDA C Programming Guide.” NVIDIA Developer Documentation. Accessed September 20, 2019. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html. 

[8] “OpenCL - The Open Standard for Parallel Programming of Heterogeneous Systems.” The Khronos Group, July 21, 2013. https://www.khronos.org/opencl/

[9] “About OpenACC.” Open ACC. Accessed September 20, 2019. https://www.openacc.org/about. 

[10] Nickolls, John, and David Kirk. "Graphics and computing GPUs." Computer Organization and Design: The Hardware/Software Interface, DA Patterson and JL Hennessy, 4th ed., Morgan Kaufmann (2009): A2-A77.

[11] "NVIDIA Professional Graphics Solutions". NVIDIA. Accessed October 11, 2019. https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/documents/quadro-mobile-line-card-n18-11x8.5-r4-hr.pdf
